<html>

  <head>
    <title>Bottom-Up and Top-Down Reasoning with Rectified Hierarchical Gaussians</title>
    <script>
      (function(i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function() {
      (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
      m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
      })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

      ga('create', 'UA-75596919-1', 'auto');
      ga('send', 'pageview');
    </script>
  </head>



  <body>
    <div style="padding: 20px; margin: 0 auto; width:70%; text-align:center">
      <h1 style="font-weight: normal; font-size: 20pt;"> 
	Bottom-Up and Top-Down Reasoning with Hierarchical Rectified Gaussians 
      </h1>

      <table align="center" width="30%" style="font-size: 14pt">
        <tr>
          <td> <a href="http://www.ics.uci.edu/~peiyunh/">Peiyun Hu</a> </td>
          <td> <a href="http://www.cs.cmu.edu/~deva/">Deva Ramanan</a> </td>
        </tr>
      </table>

      <div style="padding: 10px;">
        <a href="./teaser.png"> <img width=60% src="./teaser.png" /> </a>
      </div>

      <h3 style="font-weight: normal; font-size: 15pt;"> Abstract </h3>
      <div style="margin: 0 auto; width: 80%; text-align: left">
	<p align="justify">
          Convolutional neural nets (CNNs) have demonstrated
          remarkable performance in recent history. Such approaches
          tend to work in a unidirectional bottom-up feed-forward
          fashion. However, practical experience and biological
          evidence tells us that feedback plays a crucial role,
          particularly for detailed spatial understanding tasks. This
          work explores bidirectional architectures that also reason
          with top-down feedback: neural units are influenced by both
          lower and higher-level units.
	</p>
	<p align="justify">
	  We do so by treating units as rectified latent variables in
          a quadratic energy function, which can be seen as a
          hierarchical Rectified Gaussian model (RGs). We show that
          RGs can be optimized with a quadratic program (QP), that can
          in turn be optimized with a recurrent neural network (with
          rectified linear units). This allows RGs to be trained with
          GPU-optimized gradient descent. From a theoretical
          perspective, RGs help establish a connection between CNNs
          and hierarchical probabilistic models. From a practical
          perspective, RGs are well suited for detailed spatial tasks
          that can benefit from top-down reasoning. We illustrate them
          on the challenging task of keypoint localization under
          occlusions, where local bottom-up evidence may be
          misleading. We demonstrate state-of-the-art results on
          challenging benchmarks.
	</p>
      </div>
      <div style="padding: 10px; margin: 0 auto;" align="center">
	<h3 style="font-weight: normal; font-size: 15pt;"> Paper and Code </h3>
	<table>
	  <tbody>
	    <tr><td>
		<div>
		  <a href="./cvpr16_rg.pdf">
		    <img src="images/paper/cvpr16_rg-0.png" width="140" height="180"/>
		  </a>
		</div>
	      </td><td>
		<div style="padding: 5px">
		  See our paper <a href="./cvpr16_rg.pdf">here</a> (8.0MB).
		</div>
		<div style="padding: 5px">
		  See our supplementary material <a href="./cvpr16_rg_supp.pdf">here</a>.
		</div>
		<div style="padding: 5px">
		  See our code <a href="https://github.com/peiyunh/rg-mpii">here</a> (GitHub). 
		</div>
		<div style="padding: 5px" >
		  See our models on
		  <a href="http://www.ics.uci.edu/~peiyunh/public/rg-mpii/">MPII</a> 
		  and 
		  <a href="http://www.ics.uci.edu/~peiyunh/public/rg-aflw/">AFLW</a>. 
		</div>
	    </td></tr>
	  </tbody>
	</table>
      </div>

    </div>

  </body>

</html>
